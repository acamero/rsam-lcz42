{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a118397-9181-46aa-a443-215642d639d7",
   "metadata": {},
   "source": [
    "# Remote Sensing Advanced Methods\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Meet the Data](#2.-Meet-the-Data)\n",
    "3. [Machine Learning: Random Forest Classifier](#3.-Machine-Learning:-Random-Forest-Classifier)\n",
    "4. [Deep Learning](#4.-Deep-Learning)\n",
    "5. [Open Problems](#5.-Open-Problems)\n",
    "6. [Challenge](#6.-Challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b6d03-b57d-42c8-8959-784589111e15",
   "metadata": {},
   "source": [
    "## 1. Introduction: So2Sat LCZ42\n",
    "\n",
    "By 2050, Berlin summers could be as hot as in Canberra, Australia. Pankow, a district in the city’s north, has already declared a climate emergency in 2019 and is planning ahead. It is planting trees from the Mediterranean that can withstand the heat, and has calculated computer simulations for sunshine and cold air corridors for the construction of 1200 new apartments. A few changes, like swapping asphalt and concrete that store heat against greenery that soaks up water and provides shade, can make a difference on the local scale. Many of these changes on a local scale then make a difference on the bigger scale.\n",
    "\n",
    "To understand local climate in cities, scientists have developed the Local Climate Zone classification scheme, as part of the So2Sat project. The aim is to create a 4D urban map of the world.\n",
    "\n",
    "It differentiates between 17 zones based mainly on surface structures (such as building and tree density) as well as surface cover (green, pervious soils versus impervious grey surfaces). There are algorithms that calculate these maps from freely available satellite imagery, but there’s still room for improvement by adapting or developing suitable and advanced Convolutional Neural Network (CNN) architectures that generalise well.\n",
    "\n",
    "The outcome of So2Sat will be the first and unique global and consistent spatial data set on urban morphology (3D/4D) of settlements, and a multidisciplinary application derivate assessing population density. This is seen as a giant leap for urban geography research as well as for formation of opinions for stakeholders based on resilient data.\n",
    "\n",
    "![Data set](so2sat_dataset.png)\n",
    "\n",
    "\n",
    "\n",
    "... and this is the data set paper:\n",
    "```\n",
    "@article{zhu2020so2sat,\n",
    "  title={So2Sat LCZ42: a benchmark data set for the classification of global local climate zones [Software and Data Sets]},\n",
    "  author={Zhu, Xiao Xiang and Hu, Jingliang and Qiu, Chunping and Shi, Yilei and Kang, Jian and Mou, Lichao and Bagheri, Hossein and Haberle, Matthias and Hua, Yuansheng and Huang, Rong and others},\n",
    "  journal={IEEE Geoscience and Remote Sensing Magazine},\n",
    "  volume={8},\n",
    "  number={3},\n",
    "  pages={76--89},\n",
    "  year={2020},\n",
    "  publisher={IEEE}\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f675649-6b34-4196-9e87-99e31ff3f69b",
   "metadata": {},
   "source": [
    "## 2. Meet the Data\n",
    "\n",
    "The *So2Sat LCZ42* data set is an *Earth observation* image classification data set. It contains co-registered image patches from Sentinel-1 (10 multi-spectral bands) and Sentinel-2 (8 bands) satellite sensors, all assigned to one of the 17 *local climate zones* (LCZ) classes.\n",
    "\n",
    "The LCZ classes are as follows: \n",
    "1) compact high-rise, \n",
    "2) compact mid-rise, \n",
    "3) compact low-rise,\n",
    "4) open high-rise,\n",
    "5) open mid-rise,\n",
    "6) open low-rise,\n",
    "7) lightweight low-rise,\n",
    "8) large low-rise,\n",
    "9) sparsely built, \n",
    "10) heavy industry,\n",
    "11) dense trees,\n",
    "12) scattered tree, \n",
    "13) bush, scrub,\n",
    "14) low plants,\n",
    "15) bare rock or paved,\n",
    "16) bare soil or sand, and \n",
    "17) water\n",
    "\n",
    "The data set is split into training (352,366 images), validation (24,188) and test (24,119).\n",
    "\n",
    "It is important to note that two various pools of cities were used to build So2Sat LCZ42. 32 cities around the globe were selected to form the training set, while samples from 10 different cites were used for the validation and test set, with a geographical split (east and west).\n",
    "\n",
    "![Sample](so2sat_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558f1ff-d24e-4dba-8a0b-8cbbfd4db827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the following lines to install the required packages\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f60af4-aa5a-4380-acad-7c13f00b83fc",
   "metadata": {},
   "source": [
    "Load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ea7c9-c9e8-4243-89f2-d2e6ac6db665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d8693-4d5c-4b16-b67f-92929ebed9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should match the name of the downloaded data set\n",
    "filename = 'data/subset_lcz42.h5'\n",
    "\n",
    "dataset = h5py.File(filename, 'r')\n",
    "\n",
    "# show the content names\n",
    "print(list(dataset.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0096f3-8195-49d4-bad2-ae16bd2ad404",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c5b1b-8f81-4a41-8457-0d1605791835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labels\n",
    "labels = np.array(dataset['label'])\n",
    "\n",
    "# show the shape\n",
    "print(\"Labels shape: \" + str(labels.shape))\n",
    "\n",
    "# print the labels\n",
    "print(labels[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db190499-19a8-497b-b41f-f8f84c4c92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data distribution\n",
    "plt.hist(np.argmax(labels, axis=1), bins=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f026998-7d3c-4b1d-89e7-417ec3d0a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get a sample from each class\n",
    "class_sample = np.argmax(labels, axis=0)\n",
    "print(class_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610b29a-deaa-421f-a640-0270c038f33b",
   "metadata": {},
   "source": [
    "### Sentinel-1\n",
    "\n",
    "The data set contains the following channels:\n",
    "1) the real part of the unfiltered VH channel\n",
    "2) the imaginary part of the unfiltered VH channel\n",
    "3) the real part of the unfiltered VV channel\n",
    "4) the imaginary part of the unfiltered VV channel\n",
    "5) the intensity of the refined Lee filtered VH channel\n",
    "6) the intensity of the refined Lee filtered VV channel\n",
    "7) the real part of the refined Lee filtered covariance matrix off-diagonal element\n",
    "8) the imaginary part of the refined Lee filtered covariance matrix off-diagonal element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2628d9d-22c2-40b8-9ec7-0c5b3a077c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Sentinel-1 data\n",
    "sen1 = np.array(dataset['sen1'])\n",
    "\n",
    "print(\"Sentinel-1 shape: \" + str(sen1.shape))\n",
    "\n",
    "plt.hist(sen1[:,:,:,0].flatten(), label=\"1\", color=\"red\", alpha=0.5, bins=100)\n",
    "plt.hist(sen1[:,:,:,2].flatten(), label=\"3\", color=\"green\", alpha=0.5, bins=100)\n",
    "plt.xscale('symlog')\n",
    "plt.show()\n",
    "plt.hist(sen1[:,:,:,4].flatten(), label=\"5\", color=\"red\", alpha=0.5, bins=100)\n",
    "plt.hist(sen1[:,:,:,5].flatten(), label=\"6\", color=\"green\", alpha=0.5, bins=100)\n",
    "plt.xscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cda482-7875-4c02-8494-ce7fe7103227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_color(X):\n",
    "    VH = X[:,:,4]\n",
    "    VV = X[:,:,5]\n",
    "    \n",
    "    c1 = 10e-4;\n",
    "    c2 = 0.01;\n",
    "    c3 = 0.02;\n",
    "    c4 = 0.03;\n",
    "    c5 = 0.045;\n",
    "    c6 = 0.05;\n",
    "    c7 = 0.9;\n",
    "    c8 = 0.25;\n",
    "\n",
    "    # Enhanced or non-enhanced option (set to \"true\" if you want enhanced)\n",
    "    enhanced = True;\n",
    "\n",
    "    if (enhanced): \n",
    "        band1 = c4 + np.log(c1 - np.log(c6 / (c3 + 2.5 * VV)) + np.log(c6 / (c3 + 1.5 * VH)))\n",
    "        band2 = c6 + np.exp(c8 * (np.log(c2 + 2 * VV) + np.log(c3 + 7 * VH)))\n",
    "        band3 = 0.8 - np.log(c6 / (c5 - c7 * VV))\n",
    "    else:\n",
    "        band1 = c4 + np.log(c1 - np.log(c6 / (c3 + 2 * VV)))\n",
    "        band2 = c6 + np.exp(c8 * (np.log(c2 + 2 * VV) + np.log(c3 + 5 * VH)))\n",
    "        band3 = 1 - np.log(c6 / (c5 - c7 * VV))\n",
    "\n",
    "    band1 = band1 / (band1.max()/255.0)\n",
    "    band2 = band2 / (band2.max()/255.0)\n",
    "    band3 = band3 / (band3.max()/255.0)\n",
    "        \n",
    "    tc = np.dstack((band1, band2, band3))\n",
    "    \n",
    "    return tc.astype('uint8')\n",
    "\n",
    "plt.imshow(false_color(sen1[class_sample[0],:,:,:]))\n",
    "plt.title('Sentinel-1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896755f-88a2-4a83-85b1-a11a80e0afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(17):\n",
    "    fig.add_subplot(4, 5, i+1)\n",
    "    plt.imshow(false_color(sen1[class_sample[i],:,:,:]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d66fa1-b8a9-4c73-8917-7ccb7c424305",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "\n",
    "Implement the radar vegetation indexing for visualization of Sentinel-1 data (for the [algorithm](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-1/radar_vegetation_index_code_dual_polarimetric/), please refer to Sentinel Hub), and check some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1069287-4551-4838-8b10-743c89a59539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def calc_rvi(X):\n",
    "#    VH = X[:,:,4]\n",
    "#    VV = X[:,:,5]   \n",
    "#    \n",
    "#    dop = (VV/(VV+VH))\n",
    "#    m = 1 - dop # dop = degree of polarization\n",
    "#    radar_vegetation_index = (np.sqrt(dop))*((4*(VH))/(VV+VH))\n",
    "#    radar_vegetation_index = ((4*(VH))/(VV+VH))\n",
    "#    \n",
    "#    return radar_vegetation_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd58105-1a30-4442-835d-601cc7cd0772",
   "metadata": {},
   "source": [
    "### Sentinel-2\n",
    "\n",
    "Sentinel-2 Bands in So2Sat LCZ 42\n",
    "1) Band B2 (Blue), 10m GSD\n",
    "2) Band B3 (Green), 10m GSD\n",
    "3) Band B4 (Red), 10m GSD\n",
    "4) Band B5, upsampled to 10m from 20m GSD\n",
    "5) Band B6, upsampled to 10m from 20m GSD\n",
    "6) Band B7, upsampled to 10m from 20m GSD\n",
    "7) Band B8, 10m GSD\n",
    "8) Band B8a, upsampled to 10m from 20m GSD\n",
    "9) Band B11, upsampled to 10m from 20m GSD\n",
    "10) and Band B12, upsampled to 10m from 20m GSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489bf06-d2b7-4a62-bda7-2cae938d4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Sentinel-2 data\n",
    "sen2 = np.array(dataset['sen2'])\n",
    "\n",
    "print(\"Sentinel-2 shape: \" + str(sen2.shape))\n",
    "\n",
    "def true_color(X):\n",
    "    \"\"\" Define True Color Sentinel image\n",
    "    \n",
    "    The function returns the MinMax scaled RGB bands\n",
    "    \n",
    "    Sentinel-2 Bands in So2Sat LCZ 42\n",
    "        1) Band B2 (Blue), 10m GSD\n",
    "        2) Band B3 (Green), 10m GSD\n",
    "        3) Band B4 (Red), 10m GSD\n",
    "        4) Band B5, upsampled to 10m from 20m GSD\n",
    "        5) Band B6, upsampled to 10m from 20m GSD\n",
    "        6) Band B7, upsampled to 10m from 20m GSD\n",
    "        7) Band B8, 10m GSD\n",
    "        8) Band B8a, upsampled to 10m from 20m GSD\n",
    "        9) Band B11, upsampled to 10m from 20m GSD\n",
    "        10) and Band B12, upsampled to 10m from 20m GSD\n",
    "\n",
    "    Matplot convention RGB [0, 255]    \n",
    "    \"\"\"    \n",
    "    blue = X[:,:,0] / (X[:,:,0].max()/255.0)\n",
    "    green = X[:,:,1] / (X[:,:,1].max()/255.0)\n",
    "    red = X[:,:,2] / (X[:,:,2].max()/255.0)\n",
    "    \n",
    "    tc = np.dstack((red, green, blue))     \n",
    "    \n",
    "    return tc.astype('uint8')\n",
    "\n",
    "# show one patch\n",
    "#plt.subplot(122)\n",
    "plt.imshow(true_color(sen2[10,:,:,0:3]))\n",
    "plt.colorbar()\n",
    "plt.title('Sentinel-2')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ae229-97ef-4d8a-adc1-41ef696ee8cb",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "\n",
    "The Normalized difference vegetation index (NDVI) is a simple, but effective index for quantifying green vegetation. It normalizes green leaf scattering in Near Infra-red wavelengths with chlorophyll absorption in red wavelengths.\n",
    "\n",
    "It is defined as\n",
    "\n",
    "$$ NDVI = Index(NIR, RED) = (NIR - RED) / (NIR + RED) $$\n",
    "\n",
    "For Sentinel-2, NIR = B8 and RED = B4\n",
    "\n",
    "Implement the NDVI and check some Sentinel-2 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd75c3-657c-421c-b673-36a94cebc897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c61cd53a-2941-48c6-a3b9-b0d838b756da",
   "metadata": {},
   "source": [
    "## 3. Machine Learning: Random Forest Classifier\n",
    "\n",
    "Machine learning: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E (*Tom M. Michell, 1997*).\n",
    "\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "![Random Forest](random-forest.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69ca9c-5b49-4d33-8bb0-acd930758871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random forest expects a vector of features. Therefore,\n",
    "# we concatenate all bands and pixels\n",
    "number_samples = sen2.shape[0]\n",
    "X = np.reshape(sen2, (number_samples, 32 * 32 * 10))\n",
    "\n",
    "print(\"Post-processed Sentinel-2 data shape: \", X.shape)\n",
    "\n",
    "# Let us split the data into train and test\n",
    "TRAIN_SPLIT = int(number_samples * .8)\n",
    "\n",
    "X_train = X[:TRAIN_SPLIT,:]\n",
    "X_test = X[TRAIN_SPLIT:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1206d3-75e4-4eda-a956-3e619963e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels are one hot encoded, but the random forest requires\n",
    "# the class number\n",
    "y_train = np.argmax(labels[0:TRAIN_SPLIT,:], axis=1)\n",
    "y_test = np.argmax(labels[TRAIN_SPLIT:2400,:], axis=1)\n",
    "\n",
    "print(\"Post-processed train labels shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e04b0-16cd-4759-b243-331c75aa01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=0)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a580f-0082-4dbc-a738-34b02def1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecdc6a0-d74a-487e-8910-5dd6a7d9d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf)\n",
    "plt.show()\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy Random Forest Classifier: \", acc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750786e-9f79-400f-aa9f-bc1d05a27834",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "\n",
    "Train a random forest to predict the local climate zone using Sentinel-1 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5f919-e403-4046-abe8-f929ca810e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sen1.shape)\n",
    "number_samples = sen1.shape[0]\n",
    "Z = np.reshape(sen1, (number_samples, 32 * 32 * 8))\n",
    "\n",
    "# Continue..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995afa99-02f6-47f5-ab78-7ee62141a389",
   "metadata": {},
   "source": [
    "## 4. Deep Learning\n",
    "\n",
    "![Deep Learning](deep-learning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05521b3-9a1e-4763-8633-c24414409d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train = sen2[:TRAIN_SPLIT,:,:,:]\n",
    "Z_test = sen2[TRAIN_SPLIT:,:,:,:]\n",
    "\n",
    "print(\"Train shape: \", Z_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2eab2b-4499-412b-97e7-16f960cdcad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(32, 32, 10)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(17, activation='softmax')\n",
    "])\n",
    "\n",
    "simple_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366db098-3147-460b-ae31-5f41231a5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.fit(Z_train, y_train, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8653350-daef-49b4-b989-8b185dcd6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl_p = simple_model.predict(Z_test)\n",
    "\n",
    "print(\"Prediction example: \", y_pred_dl_p[0,:], \" Class: \", np.argmax(y_pred_dl_p[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cabf4-e6fe-4a21-9f4f-551026cc7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl = np.argmax(y_pred_dl_p, axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dl)\n",
    "plt.show()\n",
    "\n",
    "acc_dl = accuracy_score(y_test, y_pred_dl)\n",
    "print(\"Accuracy Simple Deep Learning model: \", acc_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300dd259-98b2-40b3-9173-2ad9aa0145be",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "\n",
    "Propose your own *simple* neural network model to predict the local climate zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9da510-1841-456c-9d73-b2fb0b4bec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may use the following code to get started...\n",
    "simple_model_2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(32, 32, 10)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(17, activation='softmax')\n",
    "])\n",
    "\n",
    "simple_model_2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "simple_model_2.fit(Z_train, y_train, epochs=5, validation_split=0.2)\n",
    "y_pred_dl_p_2 = simple_model_2.predict(Z_test)\n",
    "\n",
    "y_pred_dl_2 = np.argmax(y_pred_dl_p_2, axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dl_2)\n",
    "plt.show()\n",
    "\n",
    "acc_dl_2 = accuracy_score(y_test, y_pred_dl_2)\n",
    "print(\"Accuracy Simple Deep Learning model DIY: \", acc_dl_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ea52b-c7b8-4d32-b137-c6f1bf5ee2dd",
   "metadata": {},
   "source": [
    "### Residual Network: ResNet\n",
    "\n",
    "A residual neural network (ResNet) is an artificial neural network that imitate the pyramidal cells in the cerebral cortex. Particularly, the ResNet architecture consists of skip connections or shortcuts to jump over some layers. Typical ResNet models are implemented with double- or triple- layer skips that contain nonlinearities (ReLU) and batch normalization in between. These *cells* are known as *residual block*.\n",
    "\n",
    "![Residual Block](residual-block.png)\n",
    "\n",
    "ResNets connection weights are easier to optimize (especially for gradient descent-based optimizers) because the short cuts contribute to alleviate the vanishing gradient problem.\n",
    "\n",
    "In practice, the degradation problem (i.e., increasing the depth of a network leads to a decrease in its performance) is mitigated, and the observed performance (when the number of hidden layers increase) is much closer to the theoretical one.\n",
    "\n",
    "![Residual Network](residual-network.png)\n",
    "\n",
    "Let us design a *deeper* model, based on ResNet... We will use Keras [implementation](https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py) as our basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd24a36-9488-4cae-9f27-d142349953a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(\n",
    "    input_tensor, \n",
    "    kernel_size, \n",
    "    filters, \n",
    "    stage,\n",
    "    block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # First component of main path\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=F1, \n",
    "        kernel_size=(1, 1), \n",
    "        strides=(1,1), \n",
    "        padding='valid', \n",
    "        name=conv_name_base + '2a', \n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0))(input_tensor)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(\n",
    "        axis=3, \n",
    "        name=bn_name_base + '2a')(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    # Second component of main path\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=F2, \n",
    "        kernel_size=kernel_size, \n",
    "        strides=(1,1), \n",
    "        padding='same', \n",
    "        name=conv_name_base + '2b', \n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(\n",
    "        axis=3, \n",
    "        name=bn_name_base + '2b')(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # Third component of main path\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=F3, \n",
    "        kernel_size=(1, 1), \n",
    "        strides=(1,1), \n",
    "        padding='valid', \n",
    "        name=conv_name_base + '2c', \n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis = 3, name = bn_name_base + '2c')(x)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    x = tf.keras.layers.Add()([x, input_tensor])\n",
    "    x = tf.keras.layers.Activation('relu')(x)  \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056e458-bb3e-42ee-95d4-2a8be75ac21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(\n",
    "    input_tensor, \n",
    "    kernel_size, \n",
    "    filters, \n",
    "    stage, \n",
    "    block, \n",
    "    strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        strides: Strides for the first conv layer in the block.\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # First component of main path \n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        F1, \n",
    "        (1, 1), \n",
    "        strides = strides, \n",
    "        name = conv_name_base + '2a', \n",
    "        kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0))(input_tensor)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(\n",
    "        axis = 3, \n",
    "        name = bn_name_base + '2a')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters = F2, \n",
    "        kernel_size = kernel_size, \n",
    "        strides = (1,1), \n",
    "        padding = 'same', \n",
    "        name = conv_name_base + '2b', \n",
    "        kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0))(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(\n",
    "        axis = 3, \n",
    "        name = bn_name_base + '2b')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters = F3, \n",
    "        kernel_size = (1, 1), \n",
    "        strides = (1,1), \n",
    "        padding = 'valid', \n",
    "        name = conv_name_base + '2c', \n",
    "        kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0))(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(axis = 3, name = bn_name_base + '2c')(x)\n",
    "\n",
    "    tensor_shortcut = tf.keras.layers.Conv2D(\n",
    "        filters = F3, \n",
    "        kernel_size = (1, 1), \n",
    "        strides = strides, \n",
    "        padding = 'valid', \n",
    "        name = conv_name_base + '1',\n",
    "        kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0))(input_tensor)\n",
    "    tensor_shortcut = tf.keras.layers.BatchNormalization(\n",
    "        axis = 3, \n",
    "        name = bn_name_base + '1')(tensor_shortcut)\n",
    "    \n",
    "    x = tf.keras.layers.Add()([x, tensor_shortcut])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd9f48-a6f9-40cd-b60d-911fff167a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(32, 32, 10), classes=17):\n",
    "    # Define the input of the model\n",
    "    M_input = tf.keras.layers.Input(input_shape)\n",
    "    print(\"Input shape\", M_input.shape)\n",
    "\n",
    "    # Add zero padding to the patch\n",
    "    M = tf.keras.layers.ZeroPadding2D(padding=(3, 3))(M_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    M = tf.keras.layers.Conv2D(\n",
    "        filters=64, \n",
    "        kernel_size=(7, 7), \n",
    "        strides=(2, 2), \n",
    "        name='conv1', \n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0))(M)\n",
    "    M = tf.keras.layers.BatchNormalization(\n",
    "        axis=3, \n",
    "        name='bn_conv1')(M)\n",
    "    M = tf.keras.layers.Activation('relu')(M)\n",
    "    M = tf.keras.layers.MaxPool2D(\n",
    "        pool_size=(3, 3), \n",
    "        strides=(2, 2))(M)\n",
    "    print(\"Stage 1 shape\", M.shape)\n",
    "\n",
    "    # Stage 2\n",
    "    M = convolutional_block(\n",
    "        M, \n",
    "        kernel_size=3, \n",
    "        filters=[32, 32, 256], \n",
    "        stage=2, \n",
    "        block='a', \n",
    "        strides=(1, 1))\n",
    "    M = identity_block(M, 3, [64, 64, 256], stage=2, block='b')\n",
    "    M = identity_block(M, 3, [64, 64, 256], stage=2, block='c')\n",
    "    print(\"Stage 2 shape\", M.shape)\n",
    "\n",
    "    # Stage 3\n",
    "    M = convolutional_block(\n",
    "        M, \n",
    "        kernel_size=3, \n",
    "        filters=[128, 128, 512], \n",
    "        stage = 3, \n",
    "        block='a', \n",
    "        strides=(1, 1))\n",
    "    M = identity_block(M, 3, [128, 128, 512], stage=3, block='b')\n",
    "    M = identity_block(M, 3, [128, 128, 512], stage=3, block='c')\n",
    "    M = identity_block(M, 3, [128, 128, 512], stage=3, block='d')\n",
    "    print(\"Stage 3 shape\", M.shape)\n",
    "    \n",
    "    # Stage 4\n",
    "    M = convolutional_block(\n",
    "        M, \n",
    "        kernel_size=3, \n",
    "        filters=[256, 256, 1024], \n",
    "        stage = 4, \n",
    "        block='a', \n",
    "        strides=(2, 2))\n",
    "    M = identity_block(M, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    M = identity_block(M, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    M = identity_block(M, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    M = identity_block(M, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    M = identity_block(M, 3, [256, 256, 1024], stage=4, block='f')\n",
    "    print(\"Stage 4 shape\", M.shape)\n",
    "\n",
    "    # Stage 5\n",
    "    M = convolutional_block(\n",
    "        M, \n",
    "        kernel_size=3, \n",
    "        filters=[512, 512, 2048], \n",
    "        stage = 5, \n",
    "        block='a', \n",
    "        strides=(2, 2))\n",
    "    M = identity_block(M, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    M = identity_block(M, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    print(\"Stage 5 shape\", M.shape)\n",
    "\n",
    "    # AVGPOOL\n",
    "    M = tf.keras.layers.AveragePooling2D((2,2), name=\"avg_pool\")(M)\n",
    "    print(\"Avg pool shape\", M.shape)\n",
    "    \n",
    "    # output layer\n",
    "    M = tf.keras.layers.Flatten()(M)\n",
    "    M = tf.keras.layers.Dense(\n",
    "        classes, \n",
    "        activation='softmax', \n",
    "        name='fc' + str(classes), \n",
    "        kernel_initializer = tf.keras.initializers.GlorotUniform)(M)\n",
    "    print(\"Output shape\", M.shape)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs = M_input, outputs = M, name='ResNet50')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c8456-a5a0-4ed6-97b8-fcd90ff11f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn = ResNet50(input_shape=(32, 32, 10), classes=17)\n",
    "\n",
    "model_rn.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Uncomment the following line to check the full model\n",
    "model_rn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de618a2a-5544-42e7-914d-2cc75d49e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn.fit(Z_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b9654-c7a1-403b-a5ae-571ad1dd6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rn_p = model_rn.predict(Z_test)\n",
    "\n",
    "y_pred_rn = np.argmax(y_pred_rn_p, axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rn)\n",
    "plt.show()\n",
    "\n",
    "acc_rn = accuracy_score(y_test, y_pred_rn)\n",
    "print(\"Accuracy ResNet-50 model: \", acc_rn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd570d-81dc-4b7a-a47c-33c711d0aca4",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "\n",
    "Play around with the ResNet50 model, e.g., by modifying the training process, and/or changing the training budget (number of epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca076256-8d30-4769-b7ea-03adb27a46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn_diy = ResNet50(input_shape=(32, 32, 10), classes=17)\n",
    "\n",
    "# let's define a schedule to lower the learning rate\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "model_rn_diy.compile(\n",
    "    # If you want to learn more about the Adam algorithm, \n",
    "    # visit https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/optimizers/Adam\n",
    "    # or check Kingma et al. (2014) paper http://arxiv.org/abs/1412.6980\n",
    "    optimizer=tf.keras.optimizers.Adam(        \n",
    "        learning_rate=lr_schedule, # 0.001 is the standard value\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_rn_diy.fit(Z_train, y_train, epochs=5)\n",
    "\n",
    "y_pred_rn_p_diy = model_rn_diy.predict(Z_test)\n",
    "\n",
    "y_pred_rn_diy = np.argmax(y_pred_rn_p_diy, axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rn_diy)\n",
    "plt.show()\n",
    "\n",
    "acc_rn_diy = accuracy_score(y_test, y_pred_rn_diy)\n",
    "print(\"Accuracy ResNet-50 model: \", acc_rn_diy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef0a95-686f-405d-9efd-f63f5fac25ed",
   "metadata": {},
   "source": [
    "How about using Sentinel-1 and -2 at the same time? Check the code, and try to improve the performance using what you have learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf159882-620c-4594-aa0e-2ed3328ccfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_train = np.array(\n",
    "    [np.dstack((s1, s2)) for s1, s2 in zip(sen1[:TRAIN_SPLIT,:,:,:], sen2[:TRAIN_SPLIT,:,:,:])])\n",
    "\n",
    "fusion_test = np.array(\n",
    "    [np.dstack((s1, s2)) for s1, s2 in zip(sen1[TRAIN_SPLIT:,:,:,:], sen2[TRAIN_SPLIT:,:,:,:])])\n",
    "\n",
    "print(\"Original shape: \", sen1.shape, sen2.shape)\n",
    "print(\"Train shape: \", fusion_train.shape, y_train.shape)\n",
    "print(\"Test shape: \", fusion_test.shape, y_test.shape)\n",
    "\n",
    "model_rn_fusion = ResNet50(input_shape=(32, 32, 18), classes=17)\n",
    "\n",
    "model_rn_fusion.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_rn_fusion.fit(fusion_train, y_train, epochs=5)\n",
    "\n",
    "y_pred_rn_fusion_p = model_rn_fusion.predict(fusion_test)\n",
    "\n",
    "y_pred_rn_fusion = np.argmax(y_pred_rn_fusion_p, axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rn_fusion)\n",
    "plt.show()\n",
    "\n",
    "acc_rn_fusion = accuracy_score(y_test, y_pred_rn_fusion)\n",
    "print(\"Accuracy ResNet-50 model using Sentinel-1 and 2: \", acc_rn_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345fa52c-5993-4f49-ac6a-a34da00c57b6",
   "metadata": {},
   "source": [
    "## 5. Open Problems\n",
    "\n",
    "Remote sensing data bring some new challenges for deep learning, since satellite image\n",
    "analysis raises some unique questions that translate into challenging new scientific questions. Particularly, remote sensing data is:\n",
    "\n",
    "- *Multi-modal*, thus it requires transfering learning from different modalities and (different) sensor fusion.\n",
    "- *Geo-located*, i.e., each pixel corresponds to a spatial coordinate. Thus, it can be fused with other data sources, and enables new applications, e.g., geo-located services.\n",
    "- *Quality controlled measurements*, with confidence estimates. Therefore, things like estimating the uncertainty of the prediction made by a DL model become extremely important.\n",
    "- *Time-dependent*, thus requires tailoring models.\n",
    "- *Big*, i.e., we are talking about PB of data, and in consequence efficient models are needed. Also, most data is unlabeled, thus learning with few labeled data is critical.\n",
    "- *Geo-physical or bio-chemical quantities*, so to interpret the data expert knowledge is needed.\n",
    "\n",
    "Besides these challenges, automating the whole process is needed (AutoML) but considering all the particularities; and we must consider the ethical implications of working with Earth observation data!\n",
    "\n",
    "![Open challenges](open-challenges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a1246-60bd-4cc0-9613-0a624f920d52",
   "metadata": {},
   "source": [
    "## 6. Challenge\n",
    "\n",
    "Propose the best DL model to predict LCZ!\n",
    "\n",
    "You may start from the *simple* model or the ResNet50, use Sentinel-1 and/or 2, modify the architecture by adding/removing/modifying layers, changing the optimizer parameters, ...\n",
    "\n",
    "Check Tensorflow [documentation](https://www.tensorflow.org/api_docs/python/tf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3831c-7c0a-4df5-8de8-fd580edaa8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b0862-911e-4e73-9be9-aee033bc0f8f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 770-778).\n",
    "2. LeCun, Y., Bengio, Y. and Hinton, G., 2015. Deep learning. nature, 521(7553), pp.436-444.\n",
    "3. McCarthy, J., 2007. What is artificial intelligence?\n",
    "4. Mohri, M., Rostamizadeh, A. and Talwalkar, A., 2018. Foundations of machine learning. MIT press.\n",
    "5. Zhu, X.X., Hu, J., Qiu, C., Shi, Y., Kang, J., Mou, L., Bagheri, H., Haberle, M., Hua, Y., Huang, R. and Hughes, L., 2020. So2Sat LCZ42: a benchmark data set for the classification of global local climate zones. IEEE Geoscience and Remote Sensing Magazine, 8(3), pp.76-89.\n",
    "6. Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F. and Fraundorfer, F., 2017. Deep learning in remote sensing: A comprehensive review and list of resources. IEEE Geoscience and Remote Sensing Magazine, 5(4), pp.8-36."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
